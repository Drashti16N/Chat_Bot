{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce3N50qx5Eqj"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install langchain_huggingface\n",
        "!pip install langchain_groq\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain_community\n",
        "!pip install pyPDF\n",
        "!pip install pytesseract\n",
        "!pip install pyngrok\n",
        "!pip install SpeechRecognition\n",
        "!pip install pydub\n",
        "!apt update && apt install -y tesseract-ocr\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import os\n",
        "import speech_recognition as sr\n",
        "from moviepy.editor import VideoFileClip\n",
        "from pydub import AudioSegment\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "st.title(\"ðŸ—£ï¸ Conversational AI with Multi-Modal File Uploads & ðŸ“ Chat History\")\n",
        "st.subheader(\"Upload ðŸ“„PDFs/ ðŸ“¸Images/ ðŸŽ¥Video/ ðŸ”ŠAudio and ðŸ’¬chat with their content\")\n",
        "\n",
        "# Sidebar for API Key and Model Selection\n",
        "api_key = st.sidebar.text_input(\"Enter your API key:\", type=\"password\")\n",
        "model_option = st.sidebar.selectbox(\"Select an LLM model:\", (\"Gemma2-9b-It\", \"llama-3.3\", \"deepseek-r1\"))\n",
        "\n",
        "if \"store\" not in st.session_state:\n",
        "    st.session_state.store = {}\n",
        "if \"session_titles\" not in st.session_state:\n",
        "    st.session_state.session_titles = {}\n",
        "\n",
        "def initialize_llm(api_key, model_option):\n",
        "    if model_option == \"Gemma2-9b-It\":\n",
        "        return ChatGroq(groq_api_key=api_key, model_name=\"Gemma2-9b-It\")\n",
        "    elif model_option == \"llama-3.3\":\n",
        "        return ChatGroq(groq_api_key=api_key, model_name=\"llama-3.3-70b-versatile\")\n",
        "    elif model_option == \"deepseek-r1\":\n",
        "        return ChatGroq(groq_api_key=api_key, model_name=\"deepseek-r1-distill-qwen-32b\")\n",
        "    return None\n",
        "\n",
        "# Session Management\n",
        "def manage_sessions():\n",
        "    if st.sidebar.button(\"Create New Session\"):\n",
        "        new_session_id = f\"Session_{len(st.session_state.store) + 1}\"\n",
        "        st.session_state.store[new_session_id] = ChatMessageHistory()\n",
        "        st.session_state.session_titles[new_session_id] = \"New Session\"\n",
        "    session_options = [f\"{st.session_state.session_titles[s]} ({s})\" for s in st.session_state.store.keys()]\n",
        "    return st.sidebar.selectbox(\"Select a session\", session_options)\n",
        "\n",
        "selected_session = manage_sessions()\n",
        "selected_session_id = selected_session.split(\"(\")[-1].strip(\")\") if selected_session else None\n",
        "\n",
        "def process_uploaded_files(uploaded_files, file_type):\n",
        "    documents = []\n",
        "    os.makedirs(\"temp\", exist_ok=True)\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_path = os.path.join(\"temp\", uploaded_file.name)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "\n",
        "        if file_type == \"PDF\":\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            documents.extend([Document(page_content=doc.page_content) for doc in loader.load()])\n",
        "        elif file_type == \"Image\":\n",
        "            text = pytesseract.image_to_string(Image.open(file_path))\n",
        "            documents.append(Document(page_content=text))\n",
        "        elif file_type == \"Video\":\n",
        "            audio_text = extract_audio_text(file_path)\n",
        "            documents.append(Document(page_content=audio_text))\n",
        "        elif file_type == \"Audio\":\n",
        "            audio_text = transcribe_audio(file_path)\n",
        "            documents.append(Document(page_content=audio_text))\n",
        "        os.remove(file_path)\n",
        "    return documents\n",
        "\n",
        "def extract_audio_text(video_path):\n",
        "    video_clip = VideoFileClip(video_path)\n",
        "    if video_clip.audio:\n",
        "        audio_path = \"temp/temp_audio.wav\"\n",
        "        video_clip.audio.write_audiofile(audio_path, codec=\"pcm_s16le\")\n",
        "        return transcribe_audio(audio_path)\n",
        "    return \"No audio found in the video.\"\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        try:\n",
        "            return recognizer.recognize_google(audio_data)\n",
        "        except (sr.UnknownValueError, sr.RequestError):\n",
        "            return \"Audio could not be transcribed.\"\n",
        "\n",
        "file_type = st.sidebar.selectbox(\"Select file type:\", [\"PDF\", \"Image\", \"Video\", \"Audio\"])\n",
        "uploaded_files = st.sidebar.file_uploader(\"Upload files\", type=[\"pdf\", \"png\", \"jpg\", \"jpeg\", \"mp4\", \"wav\", \"mp3\"], accept_multiple_files=True)\n",
        "\n",
        "documents = process_uploaded_files(uploaded_files, file_type) if uploaded_files else []\n",
        "\n",
        "if api_key:\n",
        "    llm = initialize_llm(api_key, model_option)\n",
        "    if documents:\n",
        "        embeddings = HuggingFaceEmbeddings()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)\n",
        "        splits = text_splitter.split_documents(documents)\n",
        "        vectorstore = Chroma.from_documents(splits, embedding=embeddings)\n",
        "        retriever = vectorstore.as_retriever()\n",
        "\n",
        "        history_aware_retriever = create_history_aware_retriever(\n",
        "            llm, retriever, ChatPromptTemplate.from_messages([\n",
        "                (\"system\", \"Reformulate the question based on chat history\"),\n",
        "                MessagesPlaceholder(\"chat_history\"),\n",
        "                (\"human\", \"{input}\")\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        question_answer_chain = create_stuff_documents_chain(llm, ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"Answer using retrieved context.\"),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\")\n",
        "        ]))\n",
        "\n",
        "        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "        def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
        "            return st.session_state.store[session_id]\n",
        "\n",
        "        user_input = st.text_input(\"Your Question\")\n",
        "        if user_input:\n",
        "            session_history = st.session_state.store[selected_session_id]\n",
        "            response = rag_chain.invoke({\"input\": user_input, \"chat_history\": session_history.messages})\n",
        "            session_history.messages.append(HumanMessage(content=user_input))\n",
        "            session_history.messages.append(AIMessage(content=response['answer']))\n",
        "            st.write(\"Assistant:\", response['answer'])\n",
        "            for message in session_history.messages:\n",
        "                st.write(f\"{'User' if isinstance(message, HumanMessage) else 'Assistant'}: {message.content}\")\n",
        "else:\n",
        "    st.warning(\"Please enter the API key.\")\n"
      ],
      "metadata": {
        "id": "RUPXeG7Y5T50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}